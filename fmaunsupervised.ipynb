{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mnist.loader import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import h5py\n",
    "import librosa\n",
    "import umap\n",
    "import soundfile as sf\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style='white', context='poster')\n",
    "\n",
    "sample_rate = 8000 #minimum 8000 for sf.write "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 second clip splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(loaded_audio, file_name, clip_length = 5):\n",
    "    chunk_samples = int(clip_length * sample_rate)\n",
    "    chunks = [loaded_audio[i:i + chunk_samples] for i in range(0, len(loaded_audio), chunk_samples)]\n",
    "    for i, chunk in enumerate(chunks): #thank u stack for this cool iterative\n",
    "        output_file = f\"fma_generated/{file_name}{i}.mp3\"\n",
    "        sf.write(output_file, chunk, sample_rate)\n",
    "    \n",
    "temp, _ = librosa.load('./fma_small/000/000002.mp3', sr=sample_rate)\n",
    "temp = librosa.util.normalize(temp)\n",
    "split(temp, '000002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total 8000 tracks\n",
    "def load_batches(root_path, sample_rate, batch_size, clip_length = 5, output_dir='./fma_generated'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    file_paths=[]\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.mp3'):\n",
    "                file_paths.append(os.path.join(root, file))\n",
    "                #print(file)\n",
    "                \n",
    "    np.random.seed(88)\n",
    "    np.random.shuffle(file_paths)\n",
    "    \n",
    "    \n",
    "    chunk_samples = int(clip_length * sample_rate) #actual number of samples per chunkclip\n",
    "    \n",
    "    \n",
    "    file_index = 0\n",
    "    batch_index = 0\n",
    "    while file_index < len(file_paths):\n",
    "        batch_clips=[]\n",
    "        bathchs=[]\n",
    "        while len(batch_clips)<batch_size and file_index < len(file_paths):\n",
    "            file_path = file_paths[file_index]\n",
    "            try:\n",
    "                #print(f\"processing {os.path.basename(file_path)}\")\n",
    "                \n",
    "                audio, _ = librosa.load(file_path, sr=sample_rate)\n",
    "                filename = os.path.splitext(os.path.basename(file_path))[0]\n",
    "                \n",
    "                clip_index = 0\n",
    "                for i in range(0,len(audio), chunk_samples):\n",
    "                    chunk = audio[i:i+chunk_samples]\n",
    "                    \n",
    "                    if len(chunk) == chunk_samples: #exactly 5 seconds\n",
    "                        clip_filename = f\"{filename}{clip_index:01d}.mp3\" #single digit index :o\n",
    "                        clip_path = os.path.join(output_dir, clip_filename)\n",
    "                        \n",
    "                        sf.write(clip_path, chunk, sample_rate)\n",
    "                        batch_clips.append(chunk.copy()) #avoid ref issues\n",
    "                        bathchs.append(clip_path)\n",
    "                        clip_index += 1\n",
    "                        \n",
    "                        if len(batch_clips) >= batch_size:\n",
    "                            break\n",
    "            except Exception as e:\n",
    "                #print(f\"error loading {file_path}: {str(e)}\")\n",
    "                continue\n",
    "            file_index+=1\n",
    "        if len(batch_clips) > 0:\n",
    "            print(f\"batch {batch_index+1} with {len(batch_clips)} clips\")\n",
    "            \n",
    "            yield batch_clips.copy(), bathchs.copy()\n",
    "            del batch_clips, bathchs\n",
    "            batch_index += 1\n",
    "            \n",
    "        else: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_generated(generated_dir='./fma_generated'):\n",
    "    if os.path.exists(generated_dir):\n",
    "        for file in tqdm(os.listdir(generated_dir)):\n",
    "            if file.endswith('.mp3'):\n",
    "                os.remove(os.path.join(generated_dir, file))\n",
    "        os.rmdir(generated_dir)\n",
    "        print(f\"deleted absolutely everything within the following. directory. {generated_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 225/5006 [00:00<00:02, 2249.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5006/5006 [00:01<00:00, 4657.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted absolutely everything within the following. directory. ./fma_generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1 with 5000 clips\n"
     ]
    }
   ],
   "source": [
    "clear_generated('./fma_generated')\n",
    "batch_size = 5000 #how many clips, not how many files r processed. btw\n",
    "big_data = load_batches('./fma_small', sample_rate, batch_size) #generator function\n",
    "batches=[]\n",
    "pathches=[]\n",
    "\n",
    "\n",
    "small_data, file_paths = next(big_data)\n",
    "batches.append(small_data)\n",
    "pathches.append(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "investigation.\n",
    "https://stackoverflow.com/questions/65160046/librosa-does-not-normalize-wave-file-between-1-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "40000\n",
      "[ 1.9267792e-09  3.9689185e-09 -5.0188205e-09 ...  2.4918647e-01\n",
      "  2.5008139e-01  1.9099995e-01]\n",
      "-1.6623706e-06\n",
      "0.124991134\n"
     ]
    }
   ],
   "source": [
    "print(len(batches[0])) #first batch is 100 clips\n",
    "print(len(batches[0][0])) #each clip is 30sec*5512 samples\n",
    "print(batches[0][0]) #each sample \n",
    "print(np.mean(batches[0][0]))\n",
    "print(np.std(batches[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose either standardization or normalization. normalize for models\n",
    "https://en.wikipedia.org/wiki/Mel-frequency_cepstrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79,)\n",
      "[ 83.84733628  83.84733628 146.83238396 146.83238396 145.98669166\n",
      "  88.321517    88.83315835   0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.          67.32294488  67.32294488  67.32294488\n",
      "  66.93519325  66.54967491  66.54967491  66.93519325   0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.          74.69948441  73.84148765  74.26924704\n",
      "  81.93226047  82.40688923  81.93226047  81.93226047  81.93226047\n",
      "   0.           0.           0.           0.        ]\n",
      "(10112,)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "def into_melspecs(audio, sample_rate):\n",
    "    mel_spec=librosa.feature.melspectrogram(y=audio, sr=sample_rate)\n",
    "    #mel_db=librosa.power_to_db(mel_spec, ref=np.max) #scale to 1.0? perhaps??\n",
    "    #librosa.display.specshow(mel_db, fmax=None, sr=sample_rate, x_axis='time', y_axis='mel')\n",
    "    #plt.colorbar(format='%+2.0f dB')\n",
    "    \n",
    "    if mel_spec.max() == mel_spec.min():\n",
    "        #i'm getting nans???\n",
    "        return False\n",
    "    \n",
    "    #mel_normalized=2 * (mel_spec - mel_spec.min()) / (mel_spec.max() - mel_spec.min()) - 1\n",
    "    flattened = mel_spec.flatten()\n",
    "    #mel_standardized = (mel_db - np.mean(mel_db)) / np.std(mel_db)\n",
    "    #flattened = mel_standardized.flatten()\n",
    "    return flattened\n",
    "\n",
    "def into_textures(mel_spec, power = 2.0): #mel-frequency cepstrum. timbre\n",
    "    if mel_spec is False or mel_spec is None:\n",
    "        return False\n",
    "    try: \n",
    "        logged = np.log(mel_spec + np.full_like(mel_spec, 1e-5))\n",
    "        dct_powered = scipy.fft.dct(np.power(logged, power))\n",
    "        normalized_textures = (dct_powered - np.mean(dct_powered)) / (np.std(dct_powered))\n",
    "        return normalized_textures\n",
    "    except Exception as e:\n",
    "        print(f\"{e}\")\n",
    "        return False\n",
    "\n",
    "def into_freqs(audio, sample_rate):\n",
    "    output, _, probabilities = librosa.pyin(audio, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'), sr=sample_rate)\n",
    "    output = np.nan_to_num(output, -1)\n",
    "    #512 step\n",
    "    return output, probabilities\n",
    "\n",
    "test, testprob = into_freqs(batches[0][3], sample_rate)\n",
    "test2 = into_textures(into_melspecs(batches[0][0], sample_rate))\n",
    "print(f\"{test.shape}\\n{test}\\n{test2.shape}\")\n",
    "#x128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtering out that one clip that's not the right length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial lengths:\n",
      " batches[0]: 5000 \n",
      " pathches[0]: 4998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "into freqs processing:  27%|██▋       | 1335/5000 [10:17<25:59,  2.35it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failure at 1334 spectrogram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "into freqs processing:  27%|██▋       | 1336/5000 [10:18<27:36,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failure at 1335 spectrogram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "into freqs processing: 100%|██████████| 5000/5000 [43:44<00:00,  1.91it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "summary!\n",
      "initial total: 5000\n",
      "filtered mels count: 4998\n",
      "filtered paths count: 4996\n"
     ]
    }
   ],
   "source": [
    "batches_mels=[]\n",
    "pathches_mels=[]\n",
    "freqs=[]\n",
    "probs=[]\n",
    "\n",
    "temp=[]\n",
    "tempfreqs=[]\n",
    "tempprobs=[]\n",
    "\n",
    "print(f'initial lengths:\\n batches[0]: {len(batches[0])} \\n pathches[0]: {len(pathches[0])}')\n",
    "for i, clip in enumerate(tqdm(batches[0], desc='into freqs processing')):\n",
    "   mel_specs = into_melspecs(clip, sample_rate)\n",
    "   mel_harmonics, probabilities = into_freqs(clip, sample_rate)\n",
    "   if mel_specs is False:\n",
    "      print(f'failure at {i} spectrogram')\n",
    "      temp.append(False)\n",
    "      tempfreqs.append(False)\n",
    "      tempprobs.append(False)\n",
    "      continue\n",
    "   mel_texture = into_textures(mel_specs)\n",
    "   if mel_texture is False:\n",
    "      print(f'failure at {i} texture')\n",
    "      temp.append(False)\n",
    "      tempfreqs.append(False)\n",
    "      tempprobs.append(False)\n",
    "      continue\n",
    "   temp.append(mel_texture) \n",
    "   tempfreqs.append(mel_harmonics)\n",
    "   tempprobs.append(probabilities)\n",
    "   \n",
    "   \n",
    "   \n",
    "batches_mels.append(temp)\n",
    "freqs.append(tempfreqs)\n",
    "probs.append(tempprobs)\n",
    "pathches_mels.append(pathches[0])\n",
    "\n",
    "lengths = [len(i) for i in batches_mels[0] if i is not False]\n",
    "most_common_length = max(set(lengths), key=lengths.count)\n",
    "filtered_paths = [path for mel, path in zip(batches_mels[0], pathches[0]) if mel is not False and len(mel) == most_common_length]\n",
    "filtered_freqs = [freq for mel, freq in zip(batches_mels[0], freqs[0]) if mel is not False and len(mel) == most_common_length]\n",
    "filtered_probs = [prob for mel, prob in zip(batches_mels[0], probs[0]) if mel is not False and len(mel) == most_common_length]\n",
    "filtered_mels = [mel for mel in batches_mels[0] if mel is not False and len(mel) == most_common_length]\n",
    "\n",
    "print(f\"\\nsummary!\")\n",
    "print(f\"initial total: {len(batches[0])}\")\n",
    "print(f\"filtered mels count: {len(filtered_mels)}\")\n",
    "print(f\"filtered paths count: {len(filtered_paths)}\")\n",
    "\n",
    "batches_mels[0] = np.array(filtered_mels, dtype=np.float32)\n",
    "freqs[0] = filtered_freqs\n",
    "probs[0] = filtered_probs\n",
    "pathches_mels[0] = filtered_paths\n",
    "freq_count = len(freqs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "import numpy as np\n",
    "def create_euclidean_both(most_common_length, freq_count):\n",
    "    most_common_length = int(most_common_length)\n",
    "    freq_count = int(freq_count)\n",
    "    @numba.njit()\n",
    "    def euclidean_timbre(a, b):\n",
    "        return np.linalg.norm(a[:most_common_length]-b[:most_common_length])\n",
    "        \n",
    "\n",
    "    @numba.njit()\n",
    "    def euclidean_freq(a, b):\n",
    "        \n",
    "        freq_dists = np.nan_to_num(np.log(a[most_common_length:most_common_length+freq_count]/b[most_common_length:most_common_length+freq_count]), nan=-1.0)\n",
    "        timbretemp = 0.0\n",
    "        freqtemp = 0.0\n",
    "        for i in range(len(freq_dists)):\n",
    "            if freq_dists[i]<0.0: #either clip was just unsure? then timbre dominates\n",
    "                timbretemp += (a[i*128]-b[i*128])**2\n",
    "                continue\n",
    "            note_dist=min(abs(freq_dists[i]%0.69315), abs(freq_dists[i]%-0.69315)) #~log 2\n",
    "            freqtemp+=(note_dist**2)*(a[most_common_length+freq_count+i])*(b[most_common_length+freq_count+i])\n",
    "            \n",
    "            #more confident pitches have more distance. but this does push noisy stuff together\n",
    "        return (np.sqrt(freqtemp)+np.sqrt(timbretemp))*128\n",
    "    @numba.njit()\n",
    "    def euclidean_both(a, b):\n",
    "        return euclidean_timbre(a, b)+euclidean_freq(a, b)\n",
    "    \n",
    "    return euclidean_both\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Desktop/coding comps/transition/venv/lib/python3.13/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Users/user/Desktop/coding comps/transition/venv/lib/python3.13/site-packages/umap/umap_.py:1857: UserWarning:\n",
      "\n",
      "custom distance metric does not return gradient; inverse_transform will be unavailable. To enable using inverse_transform method, define a distance function that returns a tuple of (distance [float], gradient [np.array])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#freq_count = len(freqs[0][0])\n",
    "combined = np.concatenate([batches_mels[0], freqs[0], probs[0]], axis=1)\n",
    "euclidean_both = create_euclidean_both(most_common_length, freq_count)\n",
    "unsup_embedding = umap.UMAP(n_neighbors=40, metric = euclidean_both).fit_transform(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thank you gpt. i would never read all this documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 4998 audio clips\n",
      "Audio files served from: /Users/user/Desktop/coding comps/transition/fma_generated\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x15baccff0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dash\n",
    "from dash import dcc, html, Input, Output\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import os\n",
    "import flask\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# After your UMAP processing and filtering...\n",
    "# Create DataFrame from your clip data\n",
    "def create_clip_dataframe(umap_embedding, pathches):\n",
    "    \"\"\"\n",
    "    Create DataFrame for clips instead of full audio files\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'UMAP1': umap_embedding[:, 0],\n",
    "        'UMAP2': umap_embedding[:, 1], \n",
    "        'Clip_Path': pathches,\n",
    "        'Original_File': [extract_original_filename(path) for path in pathches],\n",
    "        'Clip_Index': [extract_clip_index(path) for path in pathches]\n",
    "    })\n",
    "    \n",
    "    # Create audio URLs for serving\n",
    "    df['audio_url'] = df['Clip_Path'].apply(get_clip_audio_url)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_original_filename(clip_path):\n",
    "    \"\"\"Extract original filename from clip path\"\"\"\n",
    "    # ./fma_generated/000002_clip_000.mp3 -> 000002\n",
    "    basename = os.path.basename(clip_path)\n",
    "    # Remove extension and clip suffix\n",
    "    name_without_ext = os.path.splitext(basename)[0]\n",
    "    # Split on '_clip_' and take first part\n",
    "    if '_clip_' in name_without_ext:\n",
    "        return name_without_ext.split('_clip_')[0]\n",
    "    return name_without_ext\n",
    "\n",
    "def extract_clip_index(clip_path):\n",
    "    \"\"\"Extract clip index from clip path\"\"\"\n",
    "    # ./fma_generated/000002_clip_000.mp3 -> 0\n",
    "    basename = os.path.basename(clip_path)\n",
    "    name_without_ext = os.path.splitext(basename)[0]\n",
    "    if '_clip_' in name_without_ext:\n",
    "        try:\n",
    "            return int(name_without_ext.split('_clip_')[1])\n",
    "        except (ValueError, IndexError):\n",
    "            return 0\n",
    "    return 0\n",
    "\n",
    "def get_clip_audio_url(clip_path):\n",
    "    \"\"\"Convert clip path to URL for serving\"\"\"\n",
    "    # ./fma_generated/000002_clip_000.mp3 -> /audio/000002_clip_000.mp3\n",
    "    filename = os.path.basename(clip_path)\n",
    "    return f'/audio/{filename}'\n",
    "\n",
    "# Create your DataFrame\n",
    "df = create_clip_dataframe(unsup_embedding, pathches[0])\n",
    "\n",
    "# Initialize Dash app\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Set up audio serving directory (your generated clips)\n",
    "AUDIO_DIR = os.path.abspath('fma_generated')  # Changed from 'fma_small' to 'fma_generated'\n",
    "\n",
    "@app.server.route('/audio/<path:filename>')\n",
    "def serve_audio(filename):\n",
    "    \"\"\"Serve audio clips from fma_generated directory\"\"\"\n",
    "    if '..' in filename or filename.startswith('/'):\n",
    "        flask.abort(404)\n",
    "    return flask.send_from_directory(AUDIO_DIR, filename)\n",
    "\n",
    "# Create the scatter plot\n",
    "fig = px.scatter(\n",
    "    df, \n",
    "    x=\"UMAP1\", \n",
    "    y=\"UMAP2\", \n",
    "    hover_data=[\"Original_File\", \"Clip_Index\"], \n",
    "    custom_data=['audio_url'],\n",
    "    title=\"UMAP\",\n",
    "    labels={'UMAP1': 'Arbitrary Dimension 1', 'UMAP2': 'Arbitrary Dimension 2'}\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=True,\n",
    "    height=600  # Set minimum height\n",
    ")\n",
    "\n",
    "# Update hover template to show clip info\n",
    "fig.update_traces(\n",
    "    hovertemplate=\"<b>Original File:</b> %{customdata[1]}<br>\" +\n",
    "                  \"<b>Clip Index:</b> %{customdata[2]}<br>\" +\n",
    "                  \"<b>UMAP1:</b> %{x}<br>\" +\n",
    "                  \"<b>UMAP2:</b> %{y}<extra></extra>\"\n",
    ")\n",
    "\n",
    "app.layout = html.Div([\n",
    "    #html.H1(\"analysis\", style={'textAlign': 'center'}),\n",
    "    #html.P(f\"this many!!! {len(df)} audio clips\", style={'textAlign': 'center'}),\n",
    "    dcc.Graph(id=\"scatter\", figure=fig),\n",
    "    html.Div([\n",
    "        html.Audio(id='audio-player', controls=True, src=''),\n",
    "        html.Div(id='clip-info', style={'marginTop': '10px'})\n",
    "    ], style={'margin': '20px'})\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    [Output('audio-player', 'src'),\n",
    "     Output('clip-info', 'children')],\n",
    "    Input('scatter', 'clickData')\n",
    ")\n",
    "def update_audio(clickData):\n",
    "    if clickData and \"points\" in clickData:\n",
    "        audio_url = clickData[\"points\"][0][\"customdata\"][0]\n",
    "        \n",
    "        # Get point index to show clip info\n",
    "        point_idx = clickData[\"points\"][0][\"pointIndex\"]\n",
    "        clip_info = html.Div([\n",
    "            html.P(f\"file path: {df.iloc[point_idx]['Original_File']}{df.iloc[point_idx]['Clip_Index']:01d}.mp3\"),\n",
    "            html.P(f\"({df.iloc[point_idx]['UMAP1']:.3f}, {df.iloc[point_idx]['UMAP2']:.3f})\")\n",
    "        ])\n",
    "        \n",
    "        return audio_url, clip_info\n",
    "    \n",
    "    return '', html.P(\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Serving {len(df)} audio clips\")\n",
    "    print(f\"Audio files served from: {AUDIO_DIR}\")\n",
    "    app.run(debug=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "plt.scatter(unsup_embedding[:,0], unsup_embedding[:, 1])\n",
    "plt.title('unsup umap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, a typical travelling salesman problem. ILP sounds like a fun mathematical framing of the issue, so i'll get some practice of it today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
